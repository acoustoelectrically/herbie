<!doctype html>
<html><head><meta charset="utf-8"/><title>Result for 2sin (example 3.3)</title><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" integrity="sha384-9tPv11A+glH/on/wEu99NVwDPwkMQESOocs/ZGXPoIiLE8MU/qkqUcZ3zzL+6DuH" crossorigin="anonymous"/><script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js" integrity="sha384-U8Vrjwb8fuHMt6ewaCy8uqeUXv4oitYACKdB0VziCerzt011iQ/0TqlSlv8MReCm" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js" integrity="sha384-aGfk5kvhIq5x1x5YdvCp4upKZYnA8ckafviDpmWEKp4afOZEqOli7gqSnh8I6enH" crossorigin="anonymous"></script><script src="https://unpkg.com/mathjs@4.4.2/dist/math.min.js"></script><script src="https://unpkg.com/d3@6.7.0/dist/d3.min.js"></script><script src="https://unpkg.com/@observablehq/plot@0.4.3/dist/plot.umd.min.js"></script><link rel="stylesheet" type="text/css" href="../report.css"/><script src="../report.js"></script></head><body><header><h1>2sin (example 3.3)</h1><img src="../logo-car.png"/><nav><ul><li><a href="../index.html">Report</a></li><li><a href="timeline.html">Metrics</a></li></ul></nav></header><div id="large"><div>Percentage Accurate: <span class="number" title="Minimum Accuracy: 3.3% → 1.5%">62.5%<span class="unit"> → </span>99.9%</span></div><div>Time: <span class="number">19.1s</span></div><div>Alternatives: <span class="number">14</span></div><div>Speedup: <span class="number" title="Relative speed of fastest alternative that improves accuracy.">34.5×</span></div></div><section><details id="specification" class="programs"><summary><h2>Specification</h2><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#spec" target="_blank">?</a></summary><div><div id="precondition"><div class="program math">\[\left(\left(-10000 \leq x \land x \leq 10000\right) \land 10^{-16} \cdot \left|x\right| &lt; \varepsilon\right) \land \varepsilon &lt; \left|x\right|\]</div></div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\sin \left(x + \varepsilon\right) - \sin x
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (- (sin (+ x eps)) (sin x)))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return sin((x + eps)) - sin(x);
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = sin((x + eps)) - sin(x)
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return Math.sin((x + eps)) - Math.sin(x);
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return math.sin((x + eps)) - math.sin(x)
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(sin(Float64(x + eps)) - sin(x))
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = sin((x + eps)) - sin(x);
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[Sin[N[(x + eps), $MachinePrecision]], $MachinePrecision] - N[Sin[x], $MachinePrecision]), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\sin \left(x + \varepsilon\right) - \sin x
\end{array}
</pre></div></div><p>Sampling outcomes in <kbd>binary64</kbd> precision:</p><div class="bogosity"><div class="bogosity-valid" data-id="bogosity-valid" data-type="valid" data-timespan="0.006362970910377951" title="valid (0.6%)"></div><div class="bogosity-unknown" data-id="bogosity-unknown" data-type="unknown" data-timespan="0" title="unknown (0.0%)"></div><div class="bogosity-infinite" data-id="bogosity-infinite" data-type="infinite" data-timespan="0" title="infinite (0.0%)"></div><div class="bogosity-unsamplable" data-id="bogosity-unsamplable" data-type="unsamplable" data-timespan="0" title="unsamplable (0.0%)"></div><div class="bogosity-invalid" data-id="bogosity-invalid" data-type="invalid" data-timespan="0.49935711501970126" title="invalid (49.9%)"></div><div class="bogosity-precondition" data-id="bogosity-precondition" data-type="precondition" data-timespan="0.4942799140699208" title="precondition (49.4%)"></div></div></details></section><figure id="graphs"><h2>Local Percentage Accuracy vs <span id="variables"></span><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#graph" target="_blank">?</a></h2><svg></svg><div id="functions"></div><figcaption>The average percentage accuracy by input value. Horizontal axis shows value of an input variable; the variable is choosen in the title. Vertical axis is accuracy; higher is better. Red represent the original program, while blue represents Herbie's suggestion. These can be toggled with buttons below the plot. The line is an average while dots represent individual samples.</figcaption></figure><section id="cost-accuracy" class="section" data-benchmark-name="2sin (example 3.3)"><h2>Accuracy vs Speed<a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#cost-accuracy" target="_blank">?</a></h2><div class="figure-row"><svg></svg><div><p>Herbie found 14 alternatives:</p><table><thead><tr><th>Alternative</th><th class="numeric">Accuracy</th><th class="numeric">Speedup</th></tr></thead><tbody></tbody></table></div></div><figcaption>The accuracy (vertical axis) and speed (horizontal axis) of each alternatives. Up and to the right is better. The red square shows the initial program, and each blue circle shows an alternative.The line shows the best available speed-accuracy tradeoffs.</figcaption></section><section id="initial" class="programs"><h2>Initial Program: <span class="subhead"><data>62.5%</data> accurate, <data>1.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\sin \left(x + \varepsilon\right) - \sin x
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (- (sin (+ x eps)) (sin x)))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return sin((x + eps)) - sin(x);
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = sin((x + eps)) - sin(x)
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return Math.sin((x + eps)) - Math.sin(x);
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return math.sin((x + eps)) - math.sin(x)
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(sin(Float64(x + eps)) - sin(x))
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = sin((x + eps)) - sin(x);
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[Sin[N[(x + eps), $MachinePrecision]], $MachinePrecision] - N[Sin[x], $MachinePrecision]), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\sin \left(x + \varepsilon\right) - \sin x
\end{array}
</pre></div></div></section><section id="alternative1" class="programs"><h2>Alternative 1: <span class="subhead"><data>99.9%</data> accurate, <data>0.6×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\sin x \cdot \left(\left(\mathsf{fma}\left(\mathsf{fma}\left(-0.001388888888888889, \varepsilon \cdot \varepsilon, 0.041666666666666664\right), \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right) + \cos x \cdot \sin \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps)
 :precision binary64
 (+
  (*
   (sin x)
   (*
    (*
     (fma
      (fma -0.001388888888888889 (* eps eps) 0.041666666666666664)
      (* eps eps)
      -0.5)
     eps)
    eps))
  (* (cos x) (sin eps))))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return (sin(x) * ((fma(fma(-0.001388888888888889, (eps * eps), 0.041666666666666664), (eps * eps), -0.5) * eps) * eps)) + (cos(x) * sin(eps));
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(Float64(sin(x) * Float64(Float64(fma(fma(-0.001388888888888889, Float64(eps * eps), 0.041666666666666664), Float64(eps * eps), -0.5) * eps) * eps)) + Float64(cos(x) * sin(eps)))
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[Sin[x], $MachinePrecision] * N[(N[(N[(N[(-0.001388888888888889 * N[(eps * eps), $MachinePrecision] + 0.041666666666666664), $MachinePrecision] * N[(eps * eps), $MachinePrecision] + -0.5), $MachinePrecision] * eps), $MachinePrecision] * eps), $MachinePrecision]), $MachinePrecision] + N[(N[Cos[x], $MachinePrecision] * N[Sin[eps], $MachinePrecision]), $MachinePrecision]), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\sin x \cdot \left(\left(\mathsf{fma}\left(\mathsf{fma}\left(-0.001388888888888889, \varepsilon \cdot \varepsilon, 0.041666666666666664\right), \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right) + \cos x \cdot \sin \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li></li><li><p>Applied rewrites<span class="error" title="62.6% on training set">61.4%</span></p><div class="math">\[\leadsto \color{blue}{\left(\cos x \cdot \sin \varepsilon + \cos \varepsilon \cdot \sin x\right)} - \sin x
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.6% on training set">99.5%</span></p><div class="math">\[\leadsto \color{blue}{\sin x \cdot \left(\cos \varepsilon + -1\right) + \cos x \cdot \sin \varepsilon}
\]</div></li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left({\varepsilon}^{2} \cdot \left({\varepsilon}^{2} \cdot \left(\frac{1}{24} + \frac{-1}{720} \cdot {\varepsilon}^{2}\right) - \frac{1}{2}\right)\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.9% on training set">99.9%</span></p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left(\left(\mathsf{fma}\left(\mathsf{fma}\left(-0.001388888888888889, \varepsilon \cdot \varepsilon, 0.041666666666666664\right), \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative2" class="programs"><h2>Alternative 2: <span class="subhead"><data>99.8%</data> accurate, <data>0.6×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\sin x \cdot \left(\left(\mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right) + \cos x \cdot \sin \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps)
 :precision binary64
 (+
  (* (sin x) (* (* (fma (* eps eps) 0.041666666666666664 -0.5) eps) eps))
  (* (cos x) (sin eps))))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return (sin(x) * ((fma((eps * eps), 0.041666666666666664, -0.5) * eps) * eps)) + (cos(x) * sin(eps));
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(Float64(sin(x) * Float64(Float64(fma(Float64(eps * eps), 0.041666666666666664, -0.5) * eps) * eps)) + Float64(cos(x) * sin(eps)))
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[Sin[x], $MachinePrecision] * N[(N[(N[(N[(eps * eps), $MachinePrecision] * 0.041666666666666664 + -0.5), $MachinePrecision] * eps), $MachinePrecision] * eps), $MachinePrecision]), $MachinePrecision] + N[(N[Cos[x], $MachinePrecision] * N[Sin[eps], $MachinePrecision]), $MachinePrecision]), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\sin x \cdot \left(\left(\mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right) + \cos x \cdot \sin \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li></li><li><p>Applied rewrites<span class="error" title="62.6% on training set">61.4%</span></p><div class="math">\[\leadsto \color{blue}{\left(\cos x \cdot \sin \varepsilon + \cos \varepsilon \cdot \sin x\right)} - \sin x
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.6% on training set">99.5%</span></p><div class="math">\[\leadsto \color{blue}{\sin x \cdot \left(\cos \varepsilon + -1\right) + \cos x \cdot \sin \varepsilon}
\]</div></li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left({\varepsilon}^{2} \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.8% on training set">99.8%</span></p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left(\left(\mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right) \cdot \varepsilon\right) \cdot \varepsilon\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative3" class="programs"><h2>Alternative 3: <span class="subhead"><data>99.7%</data> accurate, <data>0.6×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\sin x \cdot \left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.5\right) + \cos x \cdot \sin \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps)
 :precision binary64
 (+ (* (sin x) (* (* eps eps) -0.5)) (* (cos x) (sin eps))))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return (sin(x) * ((eps * eps) * -0.5)) + (cos(x) * sin(eps));
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = (sin(x) * ((eps * eps) * (-0.5d0))) + (cos(x) * sin(eps))
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return (Math.sin(x) * ((eps * eps) * -0.5)) + (Math.cos(x) * Math.sin(eps));
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return (math.sin(x) * ((eps * eps) * -0.5)) + (math.cos(x) * math.sin(eps))
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(Float64(sin(x) * Float64(Float64(eps * eps) * -0.5)) + Float64(cos(x) * sin(eps)))
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = (sin(x) * ((eps * eps) * -0.5)) + (cos(x) * sin(eps));
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[Sin[x], $MachinePrecision] * N[(N[(eps * eps), $MachinePrecision] * -0.5), $MachinePrecision]), $MachinePrecision] + N[(N[Cos[x], $MachinePrecision] * N[Sin[eps], $MachinePrecision]), $MachinePrecision]), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\sin x \cdot \left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.5\right) + \cos x \cdot \sin \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li></li><li><p>Applied rewrites<span class="error" title="62.6% on training set">61.4%</span></p><div class="math">\[\leadsto \color{blue}{\left(\cos x \cdot \sin \varepsilon + \cos \varepsilon \cdot \sin x\right)} - \sin x
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.6% on training set">99.5%</span></p><div class="math">\[\leadsto \color{blue}{\sin x \cdot \left(\cos \varepsilon + -1\right) + \cos x \cdot \sin \varepsilon}
\]</div></li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left(\frac{-1}{2} \cdot {\varepsilon}^{2}\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.7% on training set">99.6%</span></p><div class="math">\[\leadsto \sin x \cdot \color{blue}{\left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.5\right)} + \cos x \cdot \sin \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative4" class="programs"><h2>Alternative 4: <span class="subhead"><data>99.5%</data> accurate, <data>1.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* (fma (* (sin x) -0.5) eps (cos x)) eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return fma((sin(x) * -0.5), eps, cos(x)) * eps;
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(fma(Float64(sin(x) * -0.5), eps, cos(x)) * eps)
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[(N[Sin[x], $MachinePrecision] * -0.5), $MachinePrecision] * eps + N[Cos[x], $MachinePrecision]), $MachinePrecision] * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative5" class="programs"><h2>Alternative 5: <span class="subhead"><data>99.1%</data> accurate, <data>2.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\cos x \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* (cos x) eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return cos(x) * eps;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = cos(x) * eps
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return Math.cos(x) * eps;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return math.cos(x) * eps
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(cos(x) * eps)
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = cos(x) * eps;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[Cos[x], $MachinePrecision] * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\cos x \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \cos x}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.1% on training set">98.8%</span></p><div class="math">\[\leadsto \color{blue}{\cos x \cdot \varepsilon}
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative6" class="programs"><h2>Alternative 6: <span class="subhead"><data>98.6%</data> accurate, <data>4.1×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.08333333333333333, -0.5\right) \cdot x, \varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.041666666666666664, -0.5\right), x \cdot x, 1\right)\right) \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps)
 :precision binary64
 (*
  (fma
   (* (fma (* x x) 0.08333333333333333 -0.5) x)
   eps
   (fma (fma (* x x) 0.041666666666666664 -0.5) (* x x) 1.0))
  eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return fma((fma((x * x), 0.08333333333333333, -0.5) * x), eps, fma(fma((x * x), 0.041666666666666664, -0.5), (x * x), 1.0)) * eps;
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(fma(Float64(fma(Float64(x * x), 0.08333333333333333, -0.5) * x), eps, fma(fma(Float64(x * x), 0.041666666666666664, -0.5), Float64(x * x), 1.0)) * eps)
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[(N[(N[(x * x), $MachinePrecision] * 0.08333333333333333 + -0.5), $MachinePrecision] * x), $MachinePrecision] * eps + N[(N[(N[(x * x), $MachinePrecision] * 0.041666666666666664 + -0.5), $MachinePrecision] * N[(x * x), $MachinePrecision] + 1.0), $MachinePrecision]), $MachinePrecision] * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.08333333333333333, -0.5\right) \cdot x, \varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.041666666666666664, -0.5\right), x \cdot x, 1\right)\right) \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \mathsf{fma}\left(x \cdot \left(\frac{1}{12} \cdot {x}^{2} - \frac{1}{2}\right), \varepsilon, \cos x\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.0% on training set">98.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.08333333333333333, -0.5\right) \cdot x, \varepsilon, \cos x\right) \cdot \varepsilon
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, \frac{1}{12}, \frac{-1}{2}\right) \cdot x, \varepsilon, 1 + {x}^{2} \cdot \left(\frac{1}{24} \cdot {x}^{2} - \frac{1}{2}\right)\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.6% on training set">97.7%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.08333333333333333, -0.5\right) \cdot x, \varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(x \cdot x, 0.041666666666666664, -0.5\right), x \cdot x, 1\right)\right) \cdot \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative7" class="programs"><h2>Alternative 7: <span class="subhead"><data>98.5%</data> accurate, <data>10.4×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\mathsf{fma}\left(-0.5 \cdot x, \varepsilon \cdot \left(\varepsilon + x\right), \varepsilon\right)
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (fma (* -0.5 x) (* eps (+ eps x)) eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return fma((-0.5 * x), (eps * (eps + x)), eps);
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return fma(Float64(-0.5 * x), Float64(eps * Float64(eps + x)), eps)
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(-0.5 * x), $MachinePrecision] * N[(eps * N[(eps + x), $MachinePrecision]), $MachinePrecision] + eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\mathsf{fma}\left(-0.5 \cdot x, \varepsilon \cdot \left(\varepsilon + x\right), \varepsilon\right)
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \varepsilon + \color{blue}{x \cdot \left(\frac{-1}{2} \cdot \left(\varepsilon \cdot x\right) + \frac{-1}{2} \cdot {\varepsilon}^{2}\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(-0.5 \cdot x, \color{blue}{\varepsilon \cdot \left(\varepsilon + x\right)}, \varepsilon\right)
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative8" class="programs"><h2>Alternative 8: <span class="subhead"><data>98.4%</data> accurate, <data>12.2×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Julia</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\mathsf{fma}\left(-0.5 \cdot x, x, 1\right) \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* (fma (* -0.5 x) x 1.0) eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return fma((-0.5 * x), x, 1.0) * eps;
}
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(fma(Float64(-0.5 * x), x, 1.0) * eps)
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[(-0.5 * x), $MachinePrecision] * x + 1.0), $MachinePrecision] * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\mathsf{fma}\left(-0.5 \cdot x, x, 1\right) \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \varepsilon \cdot \left(\frac{-1}{2} \cdot \sin x + \varepsilon \cdot \left(\frac{-1}{6} \cdot \cos x + \frac{1}{24} \cdot \left(\varepsilon \cdot \sin x\right)\right)\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.8% on training set">99.7%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\mathsf{fma}\left(\sin x, \mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right), \left(\cos x \cdot -0.16666666666666666\right) \cdot \varepsilon\right), \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \varepsilon \cdot \left(1 + \frac{-1}{6} \cdot {\varepsilon}^{2}\right) + \color{blue}{x \cdot \left(x \cdot \left(\frac{-1}{6} \cdot \left({\varepsilon}^{2} \cdot \left(x \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)\right) + \varepsilon \cdot \left(\frac{1}{12} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right) + {\varepsilon}^{2} \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(x \cdot x, \color{blue}{\mathsf{fma}\left(\left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.16666666666666666\right) \cdot x, \mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), \mathsf{fma}\left(0.08333333333333333, \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right)}, \varepsilon \cdot \mathsf{fma}\left(\varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), x, -0.16666666666666666 \cdot \varepsilon\right), 1\right)\right)
\]</div></li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \varepsilon \cdot \left(1 + \color{blue}{\frac{-1}{2} \cdot {x}^{2}}\right)
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.4% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(-0.5 \cdot x, x, 1\right) \cdot \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative9" class="programs"><h2>Alternative 9: <span class="subhead"><data>97.9%</data> accurate, <data>34.5×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
1 \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* 1.0 eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return 1.0 * eps;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = 1.0d0 * eps
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return 1.0 * eps;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return 1.0 * eps
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(1.0 * eps)
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = 1.0 * eps;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(1.0 * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
1 \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto 1 \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="97.9% on training set">97.3%</span></p><div class="math">\[\leadsto 1 \cdot \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative10" class="programs"><h2>Alternative 10: <span class="subhead"><data>16.2%</data> accurate, <data>34.5×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
0.125 \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* 0.125 eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return 0.125 * eps;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = 0.125d0 * eps
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return 0.125 * eps;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return 0.125 * eps
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(0.125 * eps)
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = 0.125 * eps;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(0.125 * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
0.125 \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \left(1 + x \cdot \left(\frac{-1}{2} \cdot \varepsilon + \frac{-1}{2} \cdot x\right)\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(\left(\varepsilon + x\right) \cdot -0.5, x, 1\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="16.2% on training set">16.2%</span></p><div class="math">\[\leadsto 0.125 \cdot \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative11" class="programs"><h2>Alternative 11: <span class="subhead"><data>15.6%</data> accurate, <data>34.5×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
0.0625 \cdot \varepsilon
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 (* 0.0625 eps))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return 0.0625 * eps;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = 0.0625d0 * eps
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return 0.0625 * eps;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return 0.0625 * eps
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(0.0625 * eps)
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = 0.0625 * eps;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(0.0625 * eps), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
0.0625 \cdot \varepsilon
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \frac{-1}{2} \cdot \left(\varepsilon \cdot \sin x\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.5% on training set">99.4%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\sin x \cdot -0.5, \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \left(1 + x \cdot \left(\frac{-1}{2} \cdot \varepsilon + \frac{-1}{2} \cdot x\right)\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(\left(\varepsilon + x\right) \cdot -0.5, x, 1\right) \cdot \varepsilon
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="15.6% on training set">15.5%</span></p><div class="math">\[\leadsto 0.0625 \cdot \varepsilon
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative12" class="programs"><h2>Alternative 12: <span class="subhead"><data>6.2%</data> accurate, <data>207.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
5.423710169475578 \cdot 10^{-38}
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 5.423710169475578e-38)</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return 5.423710169475578e-38;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = 5.423710169475578d-38
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return 5.423710169475578e-38;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return 5.423710169475578e-38
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return 5.423710169475578e-38
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = 5.423710169475578e-38;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := 5.423710169475578e-38
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
5.423710169475578 \cdot 10^{-38}
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \varepsilon \cdot \left(\frac{-1}{2} \cdot \sin x + \varepsilon \cdot \left(\frac{-1}{6} \cdot \cos x + \frac{1}{24} \cdot \left(\varepsilon \cdot \sin x\right)\right)\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.8% on training set">99.7%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\mathsf{fma}\left(\sin x, \mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right), \left(\cos x \cdot -0.16666666666666666\right) \cdot \varepsilon\right), \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \varepsilon \cdot \left(1 + \frac{-1}{6} \cdot {\varepsilon}^{2}\right) + \color{blue}{x \cdot \left(x \cdot \left(\frac{-1}{6} \cdot \left({\varepsilon}^{2} \cdot \left(x \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)\right) + \varepsilon \cdot \left(\frac{1}{12} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right) + {\varepsilon}^{2} \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(x \cdot x, \color{blue}{\mathsf{fma}\left(\left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.16666666666666666\right) \cdot x, \mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), \mathsf{fma}\left(0.08333333333333333, \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right)}, \varepsilon \cdot \mathsf{fma}\left(\varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), x, -0.16666666666666666 \cdot \varepsilon\right), 1\right)\right)
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="5.3% on training set">5.2%</span></p><div class="math">\[\leadsto 4.8225308641975306 \cdot 10^{-5} + \color{blue}{0.08333333333333333}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="6.2% on training set">6.0%</span></p><div class="math">\[\leadsto 5.423710169475578 \cdot 10^{-38}
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative13" class="programs"><h2>Alternative 13: <span class="subhead"><data>2.4%</data> accurate, <data>207.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
-4.8225308641975306 \cdot 10^{-5}
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 -4.8225308641975306e-5)</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return -4.8225308641975306e-5;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = -4.8225308641975306d-5
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return -4.8225308641975306e-5;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return -4.8225308641975306e-5
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return -4.8225308641975306e-5
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = -4.8225308641975306e-5;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := -4.8225308641975306e-5
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
-4.8225308641975306 \cdot 10^{-5}
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \varepsilon \cdot \left(\frac{-1}{2} \cdot \sin x + \varepsilon \cdot \left(\frac{-1}{6} \cdot \cos x + \frac{1}{24} \cdot \left(\varepsilon \cdot \sin x\right)\right)\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.8% on training set">99.7%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\mathsf{fma}\left(\sin x, \mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right), \left(\cos x \cdot -0.16666666666666666\right) \cdot \varepsilon\right), \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \varepsilon \cdot \left(1 + \frac{-1}{6} \cdot {\varepsilon}^{2}\right) + \color{blue}{x \cdot \left(x \cdot \left(\frac{-1}{6} \cdot \left({\varepsilon}^{2} \cdot \left(x \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)\right) + \varepsilon \cdot \left(\frac{1}{12} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right) + {\varepsilon}^{2} \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(x \cdot x, \color{blue}{\mathsf{fma}\left(\left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.16666666666666666\right) \cdot x, \mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), \mathsf{fma}\left(0.08333333333333333, \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right)}, \varepsilon \cdot \mathsf{fma}\left(\varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), x, -0.16666666666666666 \cdot \varepsilon\right), 1\right)\right)
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="5.3% on training set">5.2%</span></p><div class="math">\[\leadsto 4.8225308641975306 \cdot 10^{-5} + \color{blue}{0.08333333333333333}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="2.4% on training set">2.4%</span></p><div class="math">\[\leadsto -4.8225308641975306 \cdot 10^{-5}
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="alternative14" class="programs"><h2>Alternative 14: <span class="subhead"><data>2.3%</data> accurate, <data>207.0×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#alternatives" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
-0.001388888888888889
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps) :precision binary64 -0.001388888888888889)</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return -0.001388888888888889;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = -0.001388888888888889d0
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return -0.001388888888888889;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return -0.001388888888888889
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return -0.001388888888888889
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = -0.001388888888888889;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := -0.001388888888888889
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
-0.001388888888888889
\end{array}
</pre></div></div><details><summary>Derivation</summary><ol class="history"><li><p>Initial program <span class="error" title="62.5% on training set">61.2%</span></p><div class="math">\[\sin \left(x + \varepsilon\right) - \sin x
\]</div></li><li>Add Preprocessing</li><li><p>Taylor expanded in eps around 0</p><div class="math">\[\leadsto \color{blue}{\varepsilon \cdot \left(\cos x + \varepsilon \cdot \left(\frac{-1}{2} \cdot \sin x + \varepsilon \cdot \left(\frac{-1}{6} \cdot \cos x + \frac{1}{24} \cdot \left(\varepsilon \cdot \sin x\right)\right)\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="99.8% on training set">99.7%</span></p><div class="math">\[\leadsto \color{blue}{\mathsf{fma}\left(\mathsf{fma}\left(\sin x, \mathsf{fma}\left(\varepsilon \cdot \varepsilon, 0.041666666666666664, -0.5\right), \left(\cos x \cdot -0.16666666666666666\right) \cdot \varepsilon\right), \varepsilon, \cos x\right) \cdot \varepsilon}
\]</div></li><li><p>Taylor expanded in x around 0</p><div class="math">\[\leadsto \varepsilon \cdot \left(1 + \frac{-1}{6} \cdot {\varepsilon}^{2}\right) + \color{blue}{x \cdot \left(x \cdot \left(\frac{-1}{6} \cdot \left({\varepsilon}^{2} \cdot \left(x \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)\right) + \varepsilon \cdot \left(\frac{1}{12} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right) + {\varepsilon}^{2} \cdot \left(\frac{1}{24} \cdot {\varepsilon}^{2} - \frac{1}{2}\right)\right)}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="98.5% on training set">97.5%</span></p><div class="math">\[\leadsto \mathsf{fma}\left(x \cdot x, \color{blue}{\mathsf{fma}\left(\left(\left(\varepsilon \cdot \varepsilon\right) \cdot -0.16666666666666666\right) \cdot x, \mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), \mathsf{fma}\left(0.08333333333333333, \varepsilon \cdot \varepsilon, -0.5\right) \cdot \varepsilon\right)}, \varepsilon \cdot \mathsf{fma}\left(\varepsilon, \mathsf{fma}\left(\mathsf{fma}\left(0.041666666666666664, \varepsilon \cdot \varepsilon, -0.5\right), x, -0.16666666666666666 \cdot \varepsilon\right), 1\right)\right)
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="5.3% on training set">5.2%</span></p><div class="math">\[\leadsto 4.8225308641975306 \cdot 10^{-5} + \color{blue}{0.08333333333333333}
\]</div></li><li></li><li><p>Applied rewrites<span class="error" title="2.3% on training set">2.4%</span></p><div class="math">\[\leadsto -0.001388888888888889
\]</div></li><li>Add Preprocessing</li></ol></details></section><section id="target1" class="programs"><h2>Developer Target 1: <span class="subhead"><data>99.9%</data> accurate, <data>0.9×</data> speedup</span><select><option>Math</option><option>FPCore</option><option>C</option><option>Fortran</option><option>Java</option><option>Python</option><option>Julia</option><option>MATLAB</option><option>Wolfram</option><option>TeX</option></select><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#target" target="_blank">?</a></h2><div><div class="implementation" data-language="Math"><div class="program math">\[\begin{array}{l}

\\
\left(\cos \left(0.5 \cdot \left(\varepsilon - -2 \cdot x\right)\right) \cdot \sin \left(0.5 \cdot \varepsilon\right)\right) \cdot 2
\end{array}
\]</div></div><div class="implementation" data-language="FPCore"><pre class="program">
(FPCore (x eps)
 :precision binary64
 (* (* (cos (* 0.5 (- eps (* -2.0 x)))) (sin (* 0.5 eps))) 2.0))</pre></div><div class="implementation" data-language="C"><pre class="program">
double code(double x, double eps) {
	return (cos((0.5 * (eps - (-2.0 * x)))) * sin((0.5 * eps))) * 2.0;
}
</pre></div><div class="implementation" data-language="Fortran"><pre class="program">
real(8) function code(x, eps)
    real(8), intent (in) :: x
    real(8), intent (in) :: eps
    code = (cos((0.5d0 * (eps - ((-2.0d0) * x)))) * sin((0.5d0 * eps))) * 2.0d0
end function
</pre></div><div class="implementation" data-language="Java"><pre class="program">
public static double code(double x, double eps) {
	return (Math.cos((0.5 * (eps - (-2.0 * x)))) * Math.sin((0.5 * eps))) * 2.0;
}
</pre></div><div class="implementation" data-language="Python"><pre class="program">
def code(x, eps):
	return (math.cos((0.5 * (eps - (-2.0 * x)))) * math.sin((0.5 * eps))) * 2.0
</pre></div><div class="implementation" data-language="Julia"><pre class="program">
function code(x, eps)
	return Float64(Float64(cos(Float64(0.5 * Float64(eps - Float64(-2.0 * x)))) * sin(Float64(0.5 * eps))) * 2.0)
end
</pre></div><div class="implementation" data-language="MATLAB"><pre class="program">
function tmp = code(x, eps)
	tmp = (cos((0.5 * (eps - (-2.0 * x)))) * sin((0.5 * eps))) * 2.0;
end
</pre></div><div class="implementation" data-language="Wolfram"><pre class="program">
code[x_, eps_] := N[(N[(N[Cos[N[(0.5 * N[(eps - N[(-2.0 * x), $MachinePrecision]), $MachinePrecision]), $MachinePrecision]], $MachinePrecision] * N[Sin[N[(0.5 * eps), $MachinePrecision]], $MachinePrecision]), $MachinePrecision] * 2.0), $MachinePrecision]
</pre></div><div class="implementation" data-language="TeX"><pre class="program">\begin{array}{l}

\\
\left(\cos \left(0.5 \cdot \left(\varepsilon - -2 \cdot x\right)\right) \cdot \sin \left(0.5 \cdot \varepsilon\right)\right) \cdot 2
\end{array}
</pre></div></div></section><section id="reproduce"><details><summary><h2>Reproduce</h2><a class="help-button float" href="https://herbie.uwplse.org/doc/2.2/report.html#reproduction" target="_blank">?</a></summary><pre class="shell"><code>herbie shell --seed 1835695063 
(FPCore (x eps)
  :name "2sin (example 3.3)"
  :precision binary64
  :pre (and (and (and (&lt;= -10000.0 x) (&lt;= x 10000.0)) (&lt; (* 1e-16 (fabs x)) eps)) (&lt; eps (fabs x)))

  :alt
  (! :herbie-platform default (* (cos (* 1/2 (- eps (* -2 x)))) (sin (* 1/2 eps)) 2))

  (- (sin (+ x eps)) (sin x)))
</code></pre></details></section></body></html>